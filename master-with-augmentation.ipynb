{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eashan\\Anaconda3\\envs\\mask_rcnn\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug\n",
    "\n",
    "# import mask rcnn libraries\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize\n",
    "\n",
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "\n",
    "# import keras libraries\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolesDataset(Dataset):\n",
    "    \n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # we add a class that we need to classify in our case it is Holes\n",
    "        self.add_class(\"dataset\", 1, \"Holes\")\n",
    "        \n",
    "        # we concatenate the dataset_dir with /images and /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            # if is_train is True skip all images with id greater than and equal to 420\n",
    "            # roughly 80% of dataset for training\n",
    "            if is_train and int(image_id) >= 420 :\n",
    "                continue\n",
    "            \n",
    "            # if is_train is not True skip all images with id less than 420\n",
    "            if not is_train and int(image_id) < 420:\n",
    "                continue\n",
    "            \n",
    "            # declaring image path and annotations path\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training or testing\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        \n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in image\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        # we append the class_id 1 for Hole in our case to the variable\n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000001110 \n",
    "        # 000011100 011100000 000001110\n",
    "        # 000011100 011100000 000001110\n",
    "        # 000000000 011100000 000000000\n",
    "        #    1         1          1    <- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('hole'))\n",
    "        \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# Hole configuration class, you can change values of hyper parameters here\n",
    "class HolesConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"Holes_cfg\"\n",
    "    \n",
    "    # Holes class + background class\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 361\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# load the train dataset\n",
    "train_set = HolesDataset()\n",
    "train_set.load_dataset('holesdataset', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# load the test dataset\n",
    "test_set = HolesDataset()\n",
    "test_set.load_dataset('holesdataset', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# prepare config by calling the user defined confifuration class\n",
    "config = HolesConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport imgaug.augmenters as iaa\n",
    "model.train(train_set,\n",
    "            test_set,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=15,\n",
    "            layers='all',\n",
    "            augmentation = iaa.Sometimes(5/6,iaa.OneOf([\n",
    "            iaa.Fliplr(1),\n",
    "            iaa.Flipud(1),\n",
    "            iaa.Affine(rotate=(-45, 45)),\n",
    "            iaa.Affine(rotate=(-90, 90)),\n",
    "            iaa.Affine(scale=(0.5, 1.5))\n",
    "            ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConfig(Config):\n",
    "    NAME = \"hole\"\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    DETECTION_MIN_CONFIDENCE = 0.85\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "\n",
    "# evaluate_model is used to calculate mean Average Precision of the model\n",
    "def evaluate_model(dataset, model, cfg):\n",
    "    APs = list()\n",
    "    for image_id in dataset.image_ids:\n",
    "\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
    "\n",
    "        scaled_image = mold_image(image, cfg)\n",
    "\n",
    "        sample = expand_dims(scaled_image, 0)\n",
    "\n",
    "        yhat = model.detect(sample, verbose=0)\n",
    "\n",
    "        r = yhat[0]\n",
    "\n",
    "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "\n",
    "        APs.append(AP)\n",
    "    maP = mean(APs)\n",
    "    return mAP\n",
    "\n",
    "# TODO: create and prepare train and test dataset\n",
    "# TODO: load prediction configuration with mode 'inference'\n",
    "# TODO: define model\n",
    "# TODO: load model weights (your trained model weights)\n",
    "\n",
    "# evaluate model on train dataset\n",
    "train_mAP = evaluate_model(train_set, model, cfg)\n",
    "print(\"Train mAP: %.3f\" % train_mAP[0])\n",
    "\n",
    "# evaluate model on test dataset\n",
    "test_mAP = evaluate_model(test_set, model, cfg)\n",
    "print(\"Test mAP: %.3f\" % test_mAP[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConfig(Config):\n",
    "    # define the name of the configuration\n",
    "    NAME = \"holes\"\n",
    "    # number of classes (background + holes)\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    # number of training steps per epoch\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "# load prediction configuration\n",
    "cfg = PredictionConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
    "\n",
    "# load model weights\n",
    "model_path = 'your_trained_weights.h5'\n",
    "\n",
    "# load image and convert o array\n",
    "image = load_img(\"image_path.jpg\")\n",
    "image = img_to_array(image)\n",
    "\n",
    "# call the detect method on image\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# class_names = ['BG', class_id_1_name, class_id_2_name]\n",
    "class_names = ['hole']\n",
    "\n",
    "r = results[0]\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names,  r['scores'])\n",
    "model.load_weights(model_path, by_name=True) "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
